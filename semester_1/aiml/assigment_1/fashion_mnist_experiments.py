# -*- coding: utf-8 -*-
"""Fashion MNIST Experiments

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m73IwuOBLLvMHhYjvjJ6p51pzZLUWl8J
"""

# ==========================================
# CELL 1: Imports & Environment Setup
# ==========================================
import tensorflow as tf
from tensorflow import keras
import numpy as np
import matplotlib.pyplot as plt
import time
import pandas as pd
import os

# Set random seeds for reproducibility where possible
tf.random.set_seed(42)
np.random.seed(42)

print(f"TensorFlow Version: {tf.__version__}")
print("Environment Ready.")

# ==========================================
# CELL 2: Load Dataset
# ==========================================
print("\nLoading Fashion MNIST Data...")
fashion_mnist = keras.datasets.fashion_mnist
(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()

# ==========================================
# CELL 3: Data Preprocessing
# ==========================================
# 1. Validation Split (First 5000 images for validation)
X_valid, X_train = X_train_full[:5000], X_train_full[5000:]
y_valid, y_train = y_train_full[:5000], y_train_full[5000:]

# 2. Normalization (Scale pixel intensities to 0-1 range)
X_valid = X_valid / 255.0
X_train = X_train / 255.0
X_test = X_test / 255.0

# 3. Class Names
class_names = ["T-shirt/top", "Trouser", "Pullover", "Dress", "Coat",
               "Sandal", "Shirt", "Sneaker", "Bag", "Ankle boot"]

print(f"Training Shape: {X_train.shape}")
print(f"Validation Shape: {X_valid.shape}")
print(f"Test Shape: {X_test.shape}")

# ==========================================
# CELL 4: Data Exploration (Sanity Check)
# ==========================================
plt.figure(figsize=(4,4))
plt.imshow(X_train[0], cmap="binary")
plt.axis('off')
plt.title(f"Sample: {class_names[y_train[0]]}")
plt.show()

# ==========================================
# CELL 5: The Experiment Engine (Model Builder)
# ==========================================
def build_and_train_model(n_layers, n_neurons, activation, kernel_init,
                          optimizer_name, learning_rate, batch_size, epochs=10):
    """
    Builds, compiles, and trains a Keras model based on the provided hyperparameters.
    """

    # 1. Clear session to prevent memory clutter
    keras.backend.clear_session()

    # 2. Configure Optimizer
    if optimizer_name.lower() == 'sgd':
        optimizer = keras.optimizers.SGD(learning_rate=learning_rate)
    elif optimizer_name.lower() == 'adam':
        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)
    else:
        raise ValueError("Unsupported optimizer")

    # 3. Build Model (Sequential API)
    model = keras.models.Sequential()
    model.add(keras.layers.Flatten(input_shape=[28, 28]))

    # Add hidden layers dynamically
    for _ in range(n_layers):
        model.add(keras.layers.Dense(n_neurons,
                                     activation=activation,
                                     kernel_initializer=kernel_init))

    # Output Layer (Fixed: 10 neurons, softmax)
    model.add(keras.layers.Dense(10, activation="softmax"))

    # 4. Compile Model
    model.compile(loss="sparse_categorical_crossentropy",
                  optimizer=optimizer,
                  metrics=["accuracy"])

    # 5. Train Model & Time it
    start_time = time.time()
    history = model.fit(X_train, y_train,
                        epochs=epochs,
                        batch_size=batch_size,
                        validation_data=(X_valid, y_valid),
                        verbose=0) # verbose=0 to reduce clutter during loops
    end_time = time.time()

    training_time = end_time - start_time
    final_val_acc = history.history['val_accuracy'][-1]

    return history, training_time, final_val_acc

print("Builder function defined.")

# ==========================================
# CELL 6: Define Baseline Constants
# ==========================================
# These are the "standard" values we revert to when not testing a specific parameter
BASELINE = {
    'n_layers': 2,
    'n_neurons': 100,
    'activation': 'relu',
    'kernel_init': 'glorot_uniform', # Default Keras "Random"
    'optimizer_name': 'sgd',
    'learning_rate': 0.01,
    'batch_size': 32
}

# Store all results here for the final table
all_results = []
# Store history objects for plotting
histories = {}

def run_experiment(exp_name, param_name, param_values):
    """Helper to run a loop of experiments"""
    print(f"\n--- Running Experiment: {exp_name} ---")

    exp_histories = {}

    for val in param_values:
        # Start with baseline config
        config = BASELINE.copy()
        # Update the parameter we are testing
        config[param_name] = val

        print(f"Testing {param_name} = {val}...", end=" ")

        hist, duration, acc = build_and_train_model(**config)

        print(f"Done. (Time: {duration:.2f}s, Val Acc: {acc:.4f})")

        # Log result
        result_entry = {
            'Experiment': exp_name,
            'Parameter': param_name,
            'Value': str(val),
            'Val Accuracy': acc,
            'Time (s)': duration
        }
        all_results.append(result_entry)
        exp_histories[val] = hist

    histories[exp_name] = exp_histories

# Experiment A: Number of Layers
run_experiment("Exp A: Layers", "n_layers", [2, 3, 4, 5])

# Experiment B: Number of Neurons
run_experiment("Exp B: Neurons", "n_neurons", [10, 50, 100, 200, 300])

# Experiment C: Activation Function
run_experiment("Exp C: Activation", "activation", ["relu", "sigmoid"])

# Comparing default Glorot Uniform vs Random Normal
run_experiment("Exp D: Init", "kernel_init", ["glorot_uniform", "random_normal"])

# Experiment E: Optimizer
run_experiment("Exp E: Optimizer", "optimizer_name", ["sgd", "adam"])

# Experiment F: Learning Rate
run_experiment("Exp F: Learning Rate", "learning_rate", [0.1, 0.01, 0.001, 0.0001])

# Experiment G: Batch Size
run_experiment("Exp G: Batch Size", "batch_size", [16, 32, 64, 128])
print("\nAll experiments completed.")

# ==========================================
# CELL 14: Visualization
# ==========================================
def plot_results(exp_name, param_name):
    exp_data = histories[exp_name]
    plt.figure(figsize=(10, 6))

    for val, hist in exp_data.items():
        plt.plot(hist.history['val_accuracy'], label=f"{param_name}={val}")

    plt.title(f"Validation Accuracy: {exp_name}")
    plt.xlabel("Epochs")
    plt.ylabel("Accuracy")
    plt.legend()
    plt.grid(True)
    plt.show()

# Generate plots for all experiments
plot_results("Exp A: Layers", "n_layers")
plot_results("Exp B: Neurons", "n_neurons")
plot_results("Exp C: Activation", "activation")
plot_results("Exp D: Init", "kernel_init")
plot_results("Exp E: Optimizer", "optimizer_name")
plot_results("Exp F: Learning Rate", "learning_rate")
plot_results("Exp G: Batch Size", "batch_size")

# ==========================================
# CELL 15: Summary Table
# ==========================================
results_df = pd.DataFrame(all_results)
print("\n=== FINAL RESULTS TABLE ===")
# Display the table (in Colab, just typing the dataframe name prints a nice HTML table)
print(results_df.to_string())

# Optional: Export to CSV for your report
results_df.to_csv("fashion_mnist_results.csv", index=False)

# ==========================================
# CELL 16: Final Evaluation (Best Model)
# ==========================================
# Based on the results above, pick the best parameters manually or programmatically.
# For demonstration, let's assume a generally good configuration for this dataset:
best_config = {
    'n_layers': 3,
    'n_neurons': 200,
    'activation': 'relu',
    'kernel_init': 'he_normal', # Often better for ReLU
    'optimizer_name': 'adam',
    'learning_rate': 0.001,
    'batch_size': 32
}

print("\nTraining Final Model with Best Configuration...")
final_model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.Dense(best_config['n_neurons'], activation=best_config['activation'], kernel_initializer=best_config['kernel_init']),
    keras.layers.Dense(best_config['n_neurons'], activation=best_config['activation'], kernel_initializer=best_config['kernel_init']),
    keras.layers.Dense(best_config['n_neurons'], activation=best_config['activation'], kernel_initializer=best_config['kernel_init']),
    keras.layers.Dense(10, activation="softmax")
])

opt = keras.optimizers.Adam(learning_rate=best_config['learning_rate'])
final_model.compile(loss="sparse_categorical_crossentropy", optimizer=opt, metrics=["accuracy"])

final_history = final_model.fit(X_train, y_train, epochs=20, # Train a bit longer for final model
                                batch_size=best_config['batch_size'],
                                validation_data=(X_valid, y_valid),
                                verbose=1)

print("\nEvaluating on Test Set...")
test_loss, test_acc = final_model.evaluate(X_test, y_test)
print(f"Final Test Accuracy: {test_acc:.4f}")